{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oops-moment/SMAI-SEM6/blob/main/Q1_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVYaSaJXzzsC"
      },
      "source": [
        "**NN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All THE LIBRARIES:\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "7_V8H9Sza5kT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJg142l3emPa",
        "outputId": "fba0ec84-ef32-4330-a00d-874810663cf7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COLLECT THE DATA MAKE DATA LOADER:\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "dataset = CIFAR10(root='data/', download=True, transform=ToTensor())\n",
        "test_dataset = CIFAR10(root='data/', train=False, transform=ToTensor())\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4-HNNESeM__",
        "outputId": "98e9925c-cbce-401e-bf50-a89cdbfbde50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:10<00:00, 15907250.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Vjh8L8Kxsmk-"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, epochs=10):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_val_loss = float('inf')\n",
        "    early_stopping_patience=3\n",
        "    epochs_without_improvement = 0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            images, labels = batch[0].to(device), batch[1].to(device)\n",
        "            # make all the gradient zero and find the output based\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        val_loss, val_acc = evaluate(model, val_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= early_stopping_patience:\n",
        "                print(f\"No improvement in validation loss for {early_stopping_patience} epochs. Stopping training.\")\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move batch to device\n",
        "            outputs = model(images)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            # basically one out of 10 with the highest probability would correspond to output of that\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    average_loss = total_loss / len(val_loader.dataset)  # Calculate average loss over the entire validation dataset\n",
        "    return average_loss, accuracy\n"
      ],
      "metadata": {
        "id": "diyAIEaUbuBU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CIFAR10MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "        super(CIFAR10MLP, self).__init__()\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
        "        self.fc4 = nn.Linear(hidden_size3, output_size)\n",
        "        # Add batch normalization layers\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_size2)\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_size3)\n",
        "        # Add dropout layers\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "jDwoGN0qskb3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gypB-MKWkafq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f82f26d3-d2eb-46a9-912d-666b3ee45dcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Val Loss: 0.0136, Val Acc: 0.3888\n",
            "Epoch [2/30], Val Loss: 0.0125, Val Acc: 0.4372\n",
            "Epoch [3/30], Val Loss: 0.0124, Val Acc: 0.4436\n",
            "Epoch [4/30], Val Loss: 0.0116, Val Acc: 0.4790\n",
            "Epoch [5/30], Val Loss: 0.0116, Val Acc: 0.4874\n",
            "Epoch [6/30], Val Loss: 0.0113, Val Acc: 0.4864\n",
            "Epoch [7/30], Val Loss: 0.0114, Val Acc: 0.4862\n",
            "Epoch [8/30], Val Loss: 0.0118, Val Acc: 0.4644\n",
            "Epoch [9/30], Val Loss: 0.0110, Val Acc: 0.4920\n",
            "Epoch [10/30], Val Loss: 0.0110, Val Acc: 0.5024\n",
            "Epoch [11/30], Val Loss: 0.0111, Val Acc: 0.5032\n",
            "Epoch [12/30], Val Loss: 0.0111, Val Acc: 0.5010\n",
            "No improvement in validation loss for 3 epochs. Stopping training.\n"
          ]
        }
      ],
      "source": [
        "# Model parameters\n",
        "input_size = 3 * 32 * 32  # 3 channels, 32x32 image size\n",
        "hidden_size1 = 512\n",
        "hidden_size2 = 256\n",
        "hidden_size3 =128\n",
        "output_size = 10\n",
        "\n",
        "# # Initialize and train the model\n",
        "model = CIFAR10MLP(input_size, hidden_size1, hidden_size2,hidden_size3, output_size)\n",
        "model = model.to(device)\n",
        "lr=0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "train_model(model, train_loader, val_loader,optimizer,epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,test_accuracy = evaluate(model, test_loader)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKydNVMuecsh",
        "outputId": "52e8b945-ebc2-4130-8b8b-bd60e0e45d0f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1dqeys1zwAi"
      },
      "source": [
        "**CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iVgdFdC9zZB2"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CIFAR10CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10CNN, self).__init__()\n",
        "        # convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        # max pooling layers\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # fully connected layers\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # convolutional layers with batch normalization and max pooling\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        # flatten image input\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        # fully connected layers with dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize and train the model\n",
        "model1 = CIFAR10CNN()\n",
        "model1 = model1.to(device)\n",
        "lr=0.001\n",
        "optimizer = torch.optim.Adam(model1.parameters(), lr=lr)\n",
        "train_model(model1, train_loader, val_loader,optimizer,epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87CrinlSfzel",
        "outputId": "799063e6-86f0-46a2-b329-77c916687c83"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Val Loss: 0.0093, Val Acc: 0.5750\n",
            "Epoch [2/30], Val Loss: 0.0097, Val Acc: 0.5850\n",
            "Epoch [3/30], Val Loss: 0.0075, Val Acc: 0.6726\n",
            "Epoch [4/30], Val Loss: 0.0067, Val Acc: 0.7030\n",
            "Epoch [5/30], Val Loss: 0.0079, Val Acc: 0.6672\n",
            "Epoch [6/30], Val Loss: 0.0082, Val Acc: 0.6528\n",
            "Epoch [7/30], Val Loss: 0.0068, Val Acc: 0.7132\n",
            "No improvement in validation loss for 3 epochs. Stopping training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGDJj1IWAEco",
        "outputId": "c491bf7c-3ddc-469e-e776-108ed804b047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7087\n"
          ]
        }
      ],
      "source": [
        "loss1,test_accuracy1 = evaluate(model1, test_loader)\n",
        "print(f\"Test Accuracy: {test_accuracy1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFe4dM-rDXT2"
      },
      "source": [
        "**VGG16**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaEh38G6Vto2",
        "outputId": "6586b551-fba6-4d0c-ef8b-0e1b00e8ef43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Val Loss: 0.0089, Val Acc: 0.6084\n",
            "Epoch [2/30], Val Loss: 0.0082, Val Acc: 0.6416\n",
            "Epoch [3/30], Val Loss: 0.0077, Val Acc: 0.6562\n",
            "Epoch [4/30], Val Loss: 0.0076, Val Acc: 0.6668\n",
            "Epoch [5/30], Val Loss: 0.0073, Val Acc: 0.6788\n",
            "Epoch [6/30], Val Loss: 0.0072, Val Acc: 0.6850\n",
            "Epoch [7/30], Val Loss: 0.0070, Val Acc: 0.6872\n",
            "Epoch [8/30], Val Loss: 0.0069, Val Acc: 0.6912\n",
            "Epoch [9/30], Val Loss: 0.0070, Val Acc: 0.6966\n",
            "Epoch [10/30], Val Loss: 0.0068, Val Acc: 0.6994\n",
            "Epoch [11/30], Val Loss: 0.0067, Val Acc: 0.7072\n",
            "Epoch [12/30], Val Loss: 0.0066, Val Acc: 0.7074\n",
            "Epoch [13/30], Val Loss: 0.0066, Val Acc: 0.7110\n",
            "Epoch [14/30], Val Loss: 0.0066, Val Acc: 0.7122\n",
            "Epoch [15/30], Val Loss: 0.0067, Val Acc: 0.7054\n",
            "Epoch [16/30], Val Loss: 0.0066, Val Acc: 0.7094\n",
            "Epoch [17/30], Val Loss: 0.0065, Val Acc: 0.7172\n",
            "Epoch [18/30], Val Loss: 0.0066, Val Acc: 0.7140\n",
            "Epoch [19/30], Val Loss: 0.0066, Val Acc: 0.7176\n",
            "Epoch [20/30], Val Loss: 0.0065, Val Acc: 0.7172\n",
            "No improvement in validation loss for 3 epochs. Stopping training.\n",
            "Test Accuracy: 0.7181\n"
          ]
        }
      ],
      "source": [
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load the pre-trained VGG-16 model\n",
        "model2 = models.vgg16(pretrained=True)\n",
        "model2= model2.to(device)\n",
        "\n",
        "\n",
        "# Freeze all the layers in the pre-trained model\n",
        "for param in model2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the last few layers\n",
        "for param in model2.features[-4:].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Modify the last layer to match the number of classes in the CIFAR-10 dataset\n",
        "num_features = model2.classifier[6].in_features\n",
        "features = list(model2.classifier.children())[:-1] # Remove last layer\n",
        "features.extend([nn.Linear(num_features, 10)]) # Add our layer with 10 outputs\n",
        "model2.classifier = nn.Sequential(*features) # Replace the model classifier\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model2.parameters()), lr=0.001, momentum=0.9)\n",
        "model2 = model2.to(device)\n",
        "train_model(model2, train_loader, val_loader,optimizer,epochs=30)\n",
        "\n",
        "loss2,test_accuracy2 = evaluate(model2, test_loader)\n",
        "print(f\"Test Accuracy: {test_accuracy2:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasons Behind Differences in Performance:**\n",
        "\n",
        "\n",
        "*   CNNs leverage the spatial structure of images through convolutional layers, which can capture local patterns and spatial hierarchies. This allows CNNs to learn hierarchical representations of features, from simple to complex, which are crucial for image classification tasks.\n",
        "*  MLPs, on the other hand, treat images as flattened vectors, disregarding their spatial structure. This results in a loss of spatial information and makes it harder for MLPs to learn meaningful representations of images.\n",
        "\n",
        "*  \n",
        "The VGG-based model, being a deep CNN architecture pre-trained on ImageNet, has already learned a rich set of features from a diverse range of images. Fine-tuning this model on CIFAR-10 allows it to adapt these learned features to the specific characteristics of the CIFAR-10 dataset, resulting in better performance compared to training from scratch.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rIXdyAFsNpL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefits of Transfer Learning:**\n",
        "\n",
        "\n",
        "*   Transfer learning allows us to leverage knowledge gained from training on a large dataset (like ImageNet) and apply it to a different but related task (CIFAR-10).\n",
        "*   \n",
        "Instead of training from scratch, which requires a large amount of data and computational resources, transfer learning allows us to start with pre-trained models and fine-tune them on our specific dataset. This reduces training time and resource requirements significantly.\n",
        "\n",
        "\n",
        "*   \n",
        "The VGG-based model, which utilizes transfer learning, achieved higher accuracy than the MLP and CNN models. This demonstrates the effectiveness of transfer learning in improving performance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7Cf41vxSiuek"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tnez5hKGioaQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}