{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict(x) for x in X]\n",
    "        return predictions\n",
    "\n",
    "    def _predict(self, x):\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in indices]\n",
    "        most_common_label, _ = Counter(k_nearest_labels).most_common(1)[0]\n",
    "        return most_common_label\n",
    "    \n",
    "    def accuracy(self, y, y_pred):\n",
    "        accuracy = np.sum(y == y_pred) / len(y)\n",
    "        return accuracy\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7195, 21)\n",
      "(7195,)\n",
      "(7195, 21)\n",
      "(7195, 21)\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('Q1Data.csv')\n",
    "selected_features = ['MFCCs_ 1', 'MFCCs_ 2', 'MFCCs_ 3', 'MFCCs_ 4', 'MFCCs_ 5', 'MFCCs_ 6','MFCCs_ 7','MFCCs_ 8', 'MFCCs_9', 'MFCCs_10', 'MFCCs_11', 'MFCCs_12', 'MFCCs_13','MFCCs_14', 'MFCCs_15', 'MFCCs_16', 'MFCCs_17', 'MFCCs_18', 'MFCCs_19','MFCCs_20', 'MFCCs_21']\n",
    "X = data[selected_features]\n",
    "Y= data['Genus']\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "X = X.to_numpy()\n",
    "print(X.shape)\n",
    "\n",
    "# lets normalize the data for X directly X-mean/std\n",
    "X = (X - np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write  a  code  from  scratch  which  predicts  the  Genus  using  the  kNN classifier using all the 21 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9812369701181376\n"
     ]
    }
   ],
   "source": [
    "knn= KNN(k=5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy = knn.accuracy(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract 30 % of the dataset for the reference/training dataset  and  10%  as  test  dataset.  You  do  not  require  to  use  the  entire  dataset  for  this  question.  Plot  the  test  accuracy  for  k  ranging from 1,2.....,50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2158, 21)\n",
      "(720, 21)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training (30%) and test (10%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, train_size=0.3, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store accuracy values\n",
    "k_values = list(range(1, 51))\n",
    "accuracy_values = []\n",
    "\n",
    "# Loop through k values\n",
    "for k in k_values:\n",
    "    # Initialize KNN classifier with current k value\n",
    "    knn = KNN(k=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    predictions = knn.predict(X_test)\n",
    "    accuracy = knn.accuracy(y_test, predictions)\n",
    "    accuracy_values.append(accuracy)\n",
    "\n",
    "# Plot the test accuracy for different k values\n",
    "plt.plot(k_values, accuracy_values, marker='o')\n",
    "plt.title('Test Accuracy for Different k Values')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Report your observations from the above graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a modest number of neighbors (around 5-10) in the k-nearest neighbors model, accuracy tends to be favorable. This is because the model benefits from capturing local patterns without being overly influenced by noise in the data. However, as the value of k increases, accuracy starts to decline. This decrease occurs because a larger k implies a broader neighborhood, leading to oversmoothing and a loss of sensitivity to local variations. Hence, striking a balance in choosing k is crucial, ensuring that it's sufficiently small to capture local patterns while avoiding excessive smoothing that can degrade accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
