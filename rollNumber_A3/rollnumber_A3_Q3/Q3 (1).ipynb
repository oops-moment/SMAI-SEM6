{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltEQ-sUobK7N",
        "outputId": "71ae5162-7eb9-45a1-a39f-49acbe2b3796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-20 14:31:28--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
            "\n",
            "../data/aclImdb_v1. 100%[===================>]  80.23M  6.67MB/s    in 7.1s    \n",
            "\n",
            "2024-03-20 14:31:35 (11.4 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%mkdir ../data\n",
        "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def read_imdb_data(data_dir='../data/aclImdb'):\n",
        "    data = {}\n",
        "    labels = {}\n",
        "\n",
        "    for data_type in ['train', 'test']:\n",
        "        data[data_type] = {}\n",
        "        labels[data_type] = {}\n",
        "\n",
        "        for sentiment in ['pos', 'neg']:\n",
        "            data[data_type][sentiment] = []\n",
        "            labels[data_type][sentiment] = []\n",
        "\n",
        "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
        "            files = glob.glob(path)\n",
        "\n",
        "            for f in files:\n",
        "                with open(f) as review:\n",
        "                    data[data_type][sentiment].append(review.read())\n",
        "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
        "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
        "\n",
        "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
        "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
        "\n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "Vi3llbw0b9uo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, labels = read_imdb_data()\n",
        "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
        "            len(data['train']['pos']), len(data['train']['neg']),\n",
        "            len(data['test']['pos']), len(data['test']['neg'])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MdvyAZ1cZXb",
        "outputId": "c0a6f940-45e6-4bd7-c2d2-82376b4df886"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def prepare_imdb_data(data, labels):\n",
        "    #Combine positive and negative reviews and labels\n",
        "    data_train = data['train']['pos'] + data['train']['neg']\n",
        "    data_test = data['test']['pos'] + data['test']['neg']\n",
        "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
        "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
        "\n",
        "    #Shuffle reviews and corresponding labels within training and test sets\n",
        "    data_train, labels_train = shuffle(data_train, labels_train)\n",
        "    data_test, labels_test = shuffle(data_test, labels_test)\n",
        "\n",
        "    # Return a unified training data, test data, training labels, test labels\n",
        "    return data_train, data_test, labels_train, labels_test"
      ],
      "metadata": {
        "id": "WP91n_jbcblo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
        "print(len(train_X))\n",
        "print(len(train_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B-3MZJsdBXV",
        "outputId": "6aa1e5f2-4b40-4693-b4a1-fdd1e8f1dd51"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n",
            "25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step in processing the reviews is to make sure that any html tags that appear should be removed. In addition we wish to tokenize our input, that way words such as entertained and entertaining are considered the same with regard to sentiment analysis."
      ],
      "metadata": {
        "id": "XdEj0NmNd0xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary cleaning Libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n"
      ],
      "metadata": {
        "id": "ZVpEp1E-KTaS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def review_to_words(review):\n",
        "    nltk.download(\"stopwords\", quiet=True)\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
        "    words = text.split() # Split string into words\n",
        "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
        "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
        "\n",
        "    return words"
      ],
      "metadata": {
        "id": "ZGE14NCJdE9c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_X[100])\n",
        "review_to_words(train_X[100])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkh7ihayerPG",
        "outputId": "c1295355-2813-4218-ca55-0d1a824390e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'll make this short and sweet....this movie sucked!!!!!!<br /><br />I watched part 1 earlier today and thought it was one of the greatest films ever, gave it 9 out of 10 stars. So I thought perhaps part 2 and 3 would be good sequels, I was wrong. This movie bored me to death and was so different from the first one, it had the plot continue and thats it. It was like bad outtakes from part one or something.<br /><br />I love Walken, but I felt sorry for him here. I was so happy about Glenn Danzig being in this film, but don't blink you'll miss him. There was a full cast full of crappy actors and people I don't like such as Eric Roberts and Jennifer Beals. However, it was a breath of fresh air to see Ethan Embry, he's one of the funniest people on earth.<br /><br />This movie will make you like the first one a little less, so don't watch it because you feel you owe it to yourself, being a fan of part 1. I am gonna wait a few days before I watch part 3 and I pray it is better than this crap.<br /><br />The last scene of the movie with the lightning was one of the most beautiful things ever shown on film. Fast forward or skip to that if you can't stomach the first part.<br /><br />1 out of 10 stars - this was awful!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['make',\n",
              " 'short',\n",
              " 'sweet',\n",
              " 'movi',\n",
              " 'suck',\n",
              " 'watch',\n",
              " 'part',\n",
              " '1',\n",
              " 'earlier',\n",
              " 'today',\n",
              " 'thought',\n",
              " 'one',\n",
              " 'greatest',\n",
              " 'film',\n",
              " 'ever',\n",
              " 'gave',\n",
              " '9',\n",
              " '10',\n",
              " 'star',\n",
              " 'thought',\n",
              " 'perhap',\n",
              " 'part',\n",
              " '2',\n",
              " '3',\n",
              " 'would',\n",
              " 'good',\n",
              " 'sequel',\n",
              " 'wrong',\n",
              " 'movi',\n",
              " 'bore',\n",
              " 'death',\n",
              " 'differ',\n",
              " 'first',\n",
              " 'one',\n",
              " 'plot',\n",
              " 'continu',\n",
              " 'that',\n",
              " 'like',\n",
              " 'bad',\n",
              " 'outtak',\n",
              " 'part',\n",
              " 'one',\n",
              " 'someth',\n",
              " 'love',\n",
              " 'walken',\n",
              " 'felt',\n",
              " 'sorri',\n",
              " 'happi',\n",
              " 'glenn',\n",
              " 'danzig',\n",
              " 'film',\n",
              " 'blink',\n",
              " 'miss',\n",
              " 'full',\n",
              " 'cast',\n",
              " 'full',\n",
              " 'crappi',\n",
              " 'actor',\n",
              " 'peopl',\n",
              " 'like',\n",
              " 'eric',\n",
              " 'robert',\n",
              " 'jennif',\n",
              " 'beal',\n",
              " 'howev',\n",
              " 'breath',\n",
              " 'fresh',\n",
              " 'air',\n",
              " 'see',\n",
              " 'ethan',\n",
              " 'embri',\n",
              " 'one',\n",
              " 'funniest',\n",
              " 'peopl',\n",
              " 'earth',\n",
              " 'movi',\n",
              " 'make',\n",
              " 'like',\n",
              " 'first',\n",
              " 'one',\n",
              " 'littl',\n",
              " 'less',\n",
              " 'watch',\n",
              " 'feel',\n",
              " 'owe',\n",
              " 'fan',\n",
              " 'part',\n",
              " '1',\n",
              " 'gonna',\n",
              " 'wait',\n",
              " 'day',\n",
              " 'watch',\n",
              " 'part',\n",
              " '3',\n",
              " 'pray',\n",
              " 'better',\n",
              " 'crap',\n",
              " 'last',\n",
              " 'scene',\n",
              " 'movi',\n",
              " 'lightn',\n",
              " 'one',\n",
              " 'beauti',\n",
              " 'thing',\n",
              " 'ever',\n",
              " 'shown',\n",
              " 'film',\n",
              " 'fast',\n",
              " 'forward',\n",
              " 'skip',\n",
              " 'stomach',\n",
              " 'first',\n",
              " 'part',\n",
              " '1',\n",
              " '10',\n",
              " 'star',\n",
              " 'aw']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "cache_dir = \"../data\"  # where to store cache files\n",
        "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
        "\n",
        "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
        "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
        "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
        "\n",
        "    # If cache_file is not None, try to read from it first\n",
        "    cache_data = None\n",
        "    if cache_file is not None:\n",
        "        try:\n",
        "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
        "                cache_data = pickle.load(f)\n",
        "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
        "        except:\n",
        "            pass  # unable to read from cache, but that's okay\n",
        "\n",
        "    # If cache is missing, then do the heavy lifting\n",
        "    if cache_data is None:\n",
        "        # Preprocess training and test data to obtain words for each review\n",
        "        #words_train = list(map(review_to_words, data_train))\n",
        "        #words_test = list(map(review_to_words, data_test))\n",
        "        words_train = [review_to_words(review) for review in data_train]\n",
        "        words_test = [review_to_words(review) for review in data_test]\n",
        "\n",
        "        # Write to cache file for future runs\n",
        "        if cache_file is not None:\n",
        "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
        "                              labels_train=labels_train, labels_test=labels_test)\n",
        "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
        "                pickle.dump(cache_data, f)\n",
        "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
        "    else:\n",
        "        # Unpack data loaded from cache file\n",
        "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
        "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
        "\n",
        "    return words_train, words_test, labels_train, labels_test\n"
      ],
      "metadata": {
        "id": "luRLaCMgeyyP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MaPkbAxwXJR",
        "outputId": "e6807e15-5c73-4500-be24-f43a6faebd0c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read preprocessed data from cache file: preprocessed_data.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QySzRBWATrRB",
        "outputId": "87f5c5fe-2307-4e99-d4fa-e484cc0b1e34"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we will be using a recurrent neural network, it will be convenient if the length of each review is the same. To do this, we will fix a size for our reviews and then pad short reviews with the category 'no word' (which we will label 0) and truncate long reviews. Basically in the dictionary the one with the most higher rank is the one that occurs most frequently. You dont care for the first two these are no words"
      ],
      "metadata": {
        "id": "__DN0F4syJkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7bgt70phy7sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def build_dict(data, vocab_size = 5000):\n",
        "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
        "    word_count = {}\n",
        "\n",
        "    for sentence in data:\n",
        "        for word in sentence:\n",
        "            if word in word_count:\n",
        "                word_count[word] += 1\n",
        "            else:\n",
        "                word_count[word] = 1\n",
        "\n",
        "    # DONE: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
        "    #       sorted_words[-1] is the least frequently appearing word.\n",
        "\n",
        "    sorted_words = sorted(word_count, key=word_count.get, reverse=True)\n",
        "    print(\"this is the first word having most frequency\",sorted_words[0])\n",
        "    print(\"this is the word having least frequency\",sorted_words[-1])\n",
        "\n",
        "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
        "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
        "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
        "\n",
        "    return word_dict"
      ],
      "metadata": {
        "id": "MPbg30bJwa_U"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "data_dir = '../data'  # The folder we will use for storing data\n",
        "word_dict_file = os.path.join(data_dir, 'word_dict.pkl')\n",
        "\n",
        "if os.path.exists(word_dict_file):\n",
        "    # If the word dictionary file exists, load it\n",
        "    with open(word_dict_file, \"rb\") as f:\n",
        "        word_dict = pickle.load(f)\n",
        "    print(\"Loaded word dictionary from:\", word_dict_file)\n",
        "else:\n",
        "    # If the word dictionary file doesn't exist, build it\n",
        "    word_dict = build_dict(train_X)\n",
        "    with open(word_dict_file, \"wb\") as f:\n",
        "        pickle.dump(word_dict, f)\n",
        "    print(\"Built and saved word dictionary to:\", word_dict_file)\n",
        "\n",
        "print(list(word_dict.keys())[0:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4vliUPYy6g5",
        "outputId": "70fb8f5f-440e-485a-8628-f504a4b1abee"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded word dictionary from: ../data/word_dict.pkl\n",
            "['movi', 'film', 'one', 'like', 'time']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our word dictionary which allows us to transform the words appearing in the reviews into integers, it is time to make use of it and convert our reviews to their integer sequence representation, making sure to pad or truncate to a fixed length, which in our case is 500."
      ],
      "metadata": {
        "id": "5FbCg7lP0kM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_and_pad(word_dict, sentence, pad=500):\n",
        "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
        "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
        "\n",
        "    working_sentence = [NOWORD] * pad\n",
        "\n",
        "    for word_index, word in enumerate(sentence[:pad]):\n",
        "        if word in word_dict:\n",
        "            working_sentence[word_index] = word_dict[word]\n",
        "        else:\n",
        "            working_sentence[word_index] = INFREQ\n",
        "\n",
        "    return working_sentence, min(len(sentence), pad)"
      ],
      "metadata": {
        "id": "FRMKuC5R1SGU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_and_pad_data(word_dict, data, pad=500):\n",
        "    result = []\n",
        "    lengths = []\n",
        "\n",
        "    for sentence in data:\n",
        "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
        "        result.append(converted)\n",
        "        lengths.append(leng)\n",
        "\n",
        "    return np.array(result), np.array(lengths)"
      ],
      "metadata": {
        "id": "6Ze7Qf851PzD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n",
        "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)\n"
      ],
      "metadata": {
        "id": "-76tjVYQ0rgd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "data_dir = '../data'  # The folder we will use for storing data\n",
        "train_csv_file = os.path.join(data_dir, 'train.csv')\n",
        "\n",
        "if not os.path.exists(train_csv_file):\n",
        "    # If the train.csv file doesn't exist, create it\n",
        "    pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\n",
        "        .to_csv(train_csv_file, header=False, index=False)\n",
        "    print(\"Created train.csv file:\", train_csv_file)\n",
        "else:\n",
        "    print(\"train.csv file already exists, skipping creation.\")"
      ],
      "metadata": {
        "id": "ymhimb1v1bK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4fdca5-e6aa-4824-865e-53226fd7d09f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.csv file already exists, skipping creation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_X[0])\n",
        "print(train_X_len[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXnpy_sr2BMI",
        "outputId": "b395d555-1bb5-44f0-f1ff-096635458685"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 660 1278    1 1880  347  601    4  329  358  306 2617  682  184  347\n",
            " 3507  841  445    6  814  213  197    1  101  797   20    4    1  284\n",
            "  373 1881  163 3533    3  640  280    1  245  287  240    1    1   60\n",
            "  410   49   52  338  181   15    5   11   79    3    1    4   54 3508\n",
            "    1 1615    1    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "    \"\"\" RNNClassifier class for initializing the layers for the simple\n",
        "    recurrent neural network model (RNN) used for Sentiment Analysis of\n",
        "    IMDB reviews.\n",
        "\n",
        "    Attributes:\n",
        "        embedding_dim (int): Dimensionality of the embedding layer\n",
        "        hidden_dim (int): Dimensionality of the hidden layer(s)\n",
        "        vocab_size (int): Size of the vocabulary used by Bag of Words\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.dense = nn.Linear(hidden_dim, 1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "        self.word_dict = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        lengths = x[:, 0]  # Extract lengths from the input\n",
        "        reviews = x[:, 1:]  # Extract reviews from the input\n",
        "        embeds = self.embedding(reviews)\n",
        "        rnn_out, _ = self.rnn(embeds)\n",
        "        out = self.dense(rnn_out[:, -1, :])  # Select the last time step's output\n",
        "        return self.sig(out.squeeze())\n"
      ],
      "metadata": {
        "id": "EiSC4oT3SfZh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    \"\"\" LSTMClassifier class for initializing the layers for the simple\n",
        "    recurrent neural network model (RNN) used for Sentiment Analysis of\n",
        "    IMDB reviews.\n",
        "\n",
        "    Attributes:\n",
        "        embedding_dim (int) dimensionality of the embedding layer\n",
        "        hidden_dim (int) dimensionality of the hidden layer(s)\n",
        "        vocab_size (int) size of the vocabulary used by Bag of Words\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
        "\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.dense = nn.Linear(in_features=hidden_dim, out_features=1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "        self.word_dict = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.t()\n",
        "        lengths = x[0,:]\n",
        "        reviews = x[1:,:]\n",
        "        embeds = self.embedding(reviews)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        out = self.dense(lstm_out)\n",
        "        out = out[lengths - 1, range(len(lengths))]\n",
        "        return self.sig(out.squeeze())"
      ],
      "metadata": {
        "id": "B8GPHB2j3DKR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "# Read in only the first 250 rows\n",
        "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None)\n",
        "\n",
        "# Turn the input pandas dataframe into tensors\n",
        "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
        "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
        "\n",
        "# Build the dataset\n",
        "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
        "# Build the dataloader\n",
        "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=100)"
      ],
      "metadata": {
        "id": "IQ-VC_tk5mGD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            batch_X, batch_y = batch\n",
        "\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            # DONE: Complete this train method to train the model provided.\n",
        "            optimizer.zero_grad()\n",
        "            output = model.forward(batch_X)\n",
        "            loss = loss_fn(output, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.data.item()\n",
        "        print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss / len(train_loader)))"
      ],
      "metadata": {
        "id": "a9YakS7Y6GGW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import os\n",
        "\n",
        "# Define the path where the trained model is saved\n",
        "model_path = '../data/trained_model.pth'\n",
        "\n",
        "# Check if the model file exists\n",
        "if os.path.exists(model_path):\n",
        "    # Load the pre-trained model\n",
        "    model = LSTMClassifier(32, 100, 5000)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print(\"Pre-trained model loaded successfully from:\", model_path)\n",
        "else:\n",
        "    # Train a new model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = LSTMClassifier(32, 100, 5000).to(device)\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    loss_fn = torch.nn.BCELoss()\n",
        "    train(model, train_sample_dl, 15, optimizer, loss_fn, device)\n",
        "    print(\"New model trained successfully.\")\n",
        "\n",
        "    # Save the trained model\n",
        "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(\"Model saved successfully at:\", model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk1Ex8gT6NBg",
        "outputId": "6ccb59ec-2b68-4e00-e44c-89bec1d579dd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-trained model loaded successfully from: ../data/trained_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([pd.DataFrame(test_y), pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1) \\\n",
        "        .to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)"
      ],
      "metadata": {
        "id": "98SRoP8_6YaE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in only the first 250 rows\n",
        "test_sample = pd.read_csv(os.path.join(data_dir, 'test.csv'), header=None, names=None)\n",
        "\n",
        "# Turn the input pandas dataframe into tensors\n",
        "test_sample_y = torch.from_numpy(test_sample[[0]].values).float().squeeze()\n",
        "test_sample_X = torch.from_numpy(test_sample.drop([0], axis=1).values).long()\n",
        "\n",
        "# Build the dataset\n",
        "test_sample_ds = torch.utils.data.TensorDataset(test_sample_X, test_sample_y)\n",
        "# Build the dataloader\n",
        "test_sample_dl = torch.utils.data.DataLoader(test_sample_ds, batch_size=100)"
      ],
      "metadata": {
        "id": "-9tn62MV88fO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, loss_fn, device):\n",
        "    \"\"\"\n",
        "    Function to evaluate the model on a given dataset.\n",
        "\n",
        "    Args:\n",
        "    - model: the PyTorch model to evaluate\n",
        "    - dataloader: DataLoader object for the dataset\n",
        "    - loss_fn: loss function used for training\n",
        "    - device: device to run the evaluation on (e.g., \"cpu\" or \"cuda\")\n",
        "\n",
        "    Returns:\n",
        "    - accuracy: accuracy of the model on the dataset\n",
        "    - loss: average loss on the dataset\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation during evaluation\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            predicted = torch.round(outputs)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "\n",
        "    return accuracy, avg_loss\n"
      ],
      "metadata": {
        "id": "r8Qa246-9sdY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, avg_loss = evaluate(model, test_sample_dl, loss_fn, device)\n",
        "print(f\"Accuracy on test set: {accuracy:.4f}\")\n",
        "print(f\"Average loss on test set: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCi40N3a-kid",
        "outputId": "a84466b7-5129-4f84-e66c-98a6c0a51b69"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 0.8629\n",
            "Average loss on test set: 0.4438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model2 = RNNClassifier(32, 100, 5000).to(device)\n",
        "optimizer = optim.Adam(model2.parameters())\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "train(model2, train_sample_dl, 15, optimizer, loss_fn, device)"
      ],
      "metadata": {
        "id": "jntgERVM-mwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8659dcc2-ee8a-46ec-a57d-01f0fddde7c9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 0.693674593925476\n",
            "Epoch: 2, Loss: 0.6926543340682983\n",
            "Epoch: 3, Loss: 0.6974989364147186\n",
            "Epoch: 4, Loss: 0.6962589757442474\n",
            "Epoch: 5, Loss: 0.6945733652114868\n",
            "Epoch: 6, Loss: 0.694539290189743\n",
            "Epoch: 7, Loss: 0.6943872539997101\n",
            "Epoch: 8, Loss: 0.6941937069892883\n",
            "Epoch: 9, Loss: 0.6944988663196564\n",
            "Epoch: 10, Loss: 0.6947130849361419\n",
            "Epoch: 11, Loss: 0.6946077134609222\n",
            "Epoch: 12, Loss: 0.6944990549087524\n",
            "Epoch: 13, Loss: 0.6943857309818268\n",
            "Epoch: 14, Loss: 0.6943688271045685\n",
            "Epoch: 15, Loss: 0.6943162977695465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, avg_loss = evaluate(model2, test_sample_dl, loss_fn, device)\n",
        "print(f\"Accuracy on test set: {accuracy:.4f}\")\n",
        "print(f\"Average loss on test set: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYCfeuIpTjAN",
        "outputId": "e493d848-463f-425e-b55b-696d3d844700"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 0.5055\n",
            "Average loss on test set: 0.6931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PgUDw68slyV7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}