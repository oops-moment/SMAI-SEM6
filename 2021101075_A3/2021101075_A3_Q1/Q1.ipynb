{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVYaSaJXzzsC"
      },
      "source": [
        "**NN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7_V8H9Sza5kT"
      },
      "outputs": [],
      "source": [
        "# All THE LIBRARIES:\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJg142l3emPa",
        "outputId": "c6392d72-9bdb-4c6a-c435-3034cb06c673"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4-HNNESeM__",
        "outputId": "c8350f17-f98e-4c1e-8b31-87689ae5bb86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 42626845.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data/\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import ToTensor, Normalize, Compose\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch\n",
        "\n",
        "# Define transformations\n",
        "transform = Compose([\n",
        "    ToTensor(),  # Convert PIL Image to tensor\n",
        "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset with transformations\n",
        "dataset = CIFAR10(root='data/', download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root='data/', train=False, transform=transform)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P6o3vGVMzQAV"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move batch to device\n",
        "            outputs = model(images)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            # basically one out of 10 with the highest probability would correspond to output of that\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    average_loss = total_loss / len(val_loader.dataset)  # Calculate average loss over the entire validation dataset\n",
        "    return average_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vjh8L8Kxsmk-"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, epochs=10):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_val_loss = float('inf')\n",
        "    early_stopping_patience=3\n",
        "    epochs_without_improvement = 0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            images, labels = batch[0].to(device), batch[1].to(device)\n",
        "            # make all the gradient zero and find the output based\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        val_loss, val_acc = evaluate(model, val_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= early_stopping_patience:\n",
        "                print(f\"No improvement in validation loss for {early_stopping_patience} epochs. Stopping training.\")\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jDwoGN0qskb3"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CIFAR10MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "        super(CIFAR10MLP, self).__init__()\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
        "        self.fc4 = nn.Linear(hidden_size3, output_size)\n",
        "        # Add batch normalization layers\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_size2)\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_size3)\n",
        "        # Add dropout layers\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n"
      ],
      "metadata": {
        "id": "Z-ZG2Qex1KyN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gypB-MKWkafq",
        "outputId": "f2c5cd4c-deec-4d2e-916e-a6ac0134368f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Val Loss: 0.0129, Val Acc: 0.4194\n",
            "Epoch [2/30], Val Loss: 0.0122, Val Acc: 0.4470\n",
            "Epoch [3/30], Val Loss: 0.0117, Val Acc: 0.4748\n",
            "Epoch [4/30], Val Loss: 0.0113, Val Acc: 0.4922\n",
            "Epoch [5/30], Val Loss: 0.0112, Val Acc: 0.4950\n",
            "Epoch [6/30], Val Loss: 0.0109, Val Acc: 0.5046\n",
            "Epoch [7/30], Val Loss: 0.0109, Val Acc: 0.5080\n",
            "Epoch [8/30], Val Loss: 0.0107, Val Acc: 0.5228\n",
            "Epoch [9/30], Val Loss: 0.0106, Val Acc: 0.5192\n",
            "Epoch [10/30], Val Loss: 0.0104, Val Acc: 0.5292\n",
            "Epoch [11/30], Val Loss: 0.0104, Val Acc: 0.5350\n",
            "Epoch [12/30], Val Loss: 0.0103, Val Acc: 0.5312\n",
            "Epoch [13/30], Val Loss: 0.0102, Val Acc: 0.5384\n",
            "Epoch [14/30], Val Loss: 0.0102, Val Acc: 0.5412\n",
            "Epoch [15/30], Val Loss: 0.0102, Val Acc: 0.5440\n",
            "Epoch [16/30], Val Loss: 0.0101, Val Acc: 0.5454\n",
            "Epoch [17/30], Val Loss: 0.0100, Val Acc: 0.5524\n",
            "Epoch [18/30], Val Loss: 0.0101, Val Acc: 0.5480\n",
            "Epoch [19/30], Val Loss: 0.0100, Val Acc: 0.5568\n",
            "Epoch [20/30], Val Loss: 0.0100, Val Acc: 0.5516\n",
            "Epoch [21/30], Val Loss: 0.0099, Val Acc: 0.5550\n",
            "Epoch [22/30], Val Loss: 0.0099, Val Acc: 0.5550\n",
            "Epoch [23/30], Val Loss: 0.0098, Val Acc: 0.5566\n",
            "Epoch [24/30], Val Loss: 0.0098, Val Acc: 0.5632\n",
            "Epoch [25/30], Val Loss: 0.0098, Val Acc: 0.5688\n",
            "Epoch [26/30], Val Loss: 0.0098, Val Acc: 0.5594\n",
            "Epoch [27/30], Val Loss: 0.0097, Val Acc: 0.5688\n",
            "Epoch [28/30], Val Loss: 0.0098, Val Acc: 0.5622\n",
            "Epoch [29/30], Val Loss: 0.0098, Val Acc: 0.5648\n",
            "Epoch [30/30], Val Loss: 0.0097, Val Acc: 0.5674\n",
            "Training completed in 387.24352502822876 seconds.\n"
          ]
        }
      ],
      "source": [
        "# Model parameters\n",
        "input_size = 3 * 32 * 32  # 3 channels, 32x32 image size\n",
        "hidden_size1 = 512\n",
        "hidden_size2 = 256\n",
        "hidden_size3 =128\n",
        "output_size = 10\n",
        "\n",
        "# # Initialize and train the model\n",
        "model = CIFAR10MLP(input_size, hidden_size1, hidden_size2,hidden_size3, output_size)\n",
        "model = model.to(device)\n",
        "lr=0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "start_time = time.time()  # Record the start time\n",
        "train_model(model, train_loader, val_loader,optimizer,epochs=30)\n",
        "end_time = time.time()  # Record the end time\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training completed in {training_time} seconds.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKydNVMuecsh",
        "outputId": "7773ece9-656c-4904-ba3d-81efa680144e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy for MLP is : 0.5639\n"
          ]
        }
      ],
      "source": [
        "loss,test_accuracy = evaluate(model, test_loader)\n",
        "print(f\"Test Accuracy for MLP is : {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1dqeys1zwAi"
      },
      "source": [
        "**CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iVgdFdC9zZB2"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CIFAR10CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        # max pooling layers\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # fully connected layers\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # convolutional layers with batch normalization and max pooling\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        # flatten image input\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        # fully connected layers with dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87CrinlSfzel",
        "outputId": "83f1d6df-a695-4edc-cdc5-c55a5ad980b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Val Loss: 0.0097, Val Acc: 0.5748\n",
            "Epoch [2/30], Val Loss: 0.0081, Val Acc: 0.6466\n",
            "Epoch [3/30], Val Loss: 0.0072, Val Acc: 0.6878\n",
            "Epoch [4/30], Val Loss: 0.0063, Val Acc: 0.7306\n",
            "Epoch [5/30], Val Loss: 0.0062, Val Acc: 0.7256\n",
            "Epoch [6/30], Val Loss: 0.0059, Val Acc: 0.7470\n",
            "Epoch [7/30], Val Loss: 0.0057, Val Acc: 0.7600\n",
            "Epoch [8/30], Val Loss: 0.0051, Val Acc: 0.7816\n",
            "Epoch [9/30], Val Loss: 0.0050, Val Acc: 0.7854\n",
            "Epoch [10/30], Val Loss: 0.0064, Val Acc: 0.7380\n",
            "Epoch [11/30], Val Loss: 0.0051, Val Acc: 0.7922\n",
            "Epoch [12/30], Val Loss: 0.0052, Val Acc: 0.7864\n",
            "No improvement in validation loss for 3 epochs. Stopping training.\n",
            "Training completed in 161.37891507148743 seconds.\n"
          ]
        }
      ],
      "source": [
        "# # Initialize and train the model\n",
        "model1 = CIFAR10CNN()\n",
        "model1 = model1.to(device)\n",
        "lr=0.001\n",
        "optimizer = torch.optim.Adam(model1.parameters(), lr=lr)\n",
        "start_time = time.time()  # Record the start time\n",
        "train_model(model1, train_loader, val_loader,optimizer,epochs=30)\n",
        "end_time = time.time()  # Record the end time\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training completed in {training_time} seconds.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGDJj1IWAEco",
        "outputId": "d513e926-9732-4364-a1bc-c361812ba812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy for CNN Model is : 0.7781\n"
          ]
        }
      ],
      "source": [
        "loss1,test_accuracy1 = evaluate(model1, test_loader)\n",
        "print(f\"Test Accuracy for CNN Model is : {test_accuracy1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFe4dM-rDXT2"
      },
      "source": [
        "**VGG16**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaEh38G6Vto2",
        "outputId": "5e256fe1-d238-43fc-ae0a-a89874907c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Val Loss: 0.0045, Val Acc: 0.8116\n",
            "Epoch [2/30], Val Loss: 0.0038, Val Acc: 0.8392\n",
            "Epoch [3/30], Val Loss: 0.0035, Val Acc: 0.8502\n",
            "Epoch [4/30], Val Loss: 0.0034, Val Acc: 0.8564\n",
            "Epoch [5/30], Val Loss: 0.0034, Val Acc: 0.8614\n",
            "Epoch [6/30], Val Loss: 0.0034, Val Acc: 0.8646\n",
            "Epoch [7/30], Val Loss: 0.0036, Val Acc: 0.8654\n",
            "Epoch [8/30], Val Loss: 0.0038, Val Acc: 0.8714\n",
            "No improvement in validation loss for 3 epochs. Stopping training.\n",
            "Training completed in 259.76842737197876 seconds.\n",
            "Test Accuracy using VGG: 0.8614\n"
          ]
        }
      ],
      "source": [
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load the pre-trained VGG-16 model\n",
        "model2 = models.vgg16(pretrained=True)\n",
        "model2= model2.to(device)\n",
        "\n",
        "\n",
        "# # Freeze all the layers in the pre-trained model\n",
        "# for param in model2.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# # Unfreeze the last few layers\n",
        "# for param in model2.features[-4:].parameters():\n",
        "#     param.requires_grad = True\n",
        "\n",
        "# Modify the last layer to match the number of classes in the CIFAR-10 dataset\n",
        "num_features = model2.classifier[6].in_features\n",
        "features = list(model2.classifier.children())[:-1] # Remove last layer\n",
        "features.extend([nn.Linear(num_features, 10)]) # Add our layer with 10 outputs\n",
        "model2.classifier = nn.Sequential(*features) # Replace the model classifier\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "# optimizer = optim.SGD(filter(lambda p: p.requires_grad, model2.parameters()), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.SGD(model2.parameters(), lr=0.001, momentum=0.9)\n",
        "model2 = model2.to(device)\n",
        "start_time = time.time()  # Record the start time\n",
        "train_model(model2, train_loader, val_loader,optimizer,epochs=30)\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training completed in {training_time} seconds.\")\n",
        "loss2,test_accuracy2 = evaluate(model2, test_loader)\n",
        "print(f\"Test Accuracy using VGG: {test_accuracy2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ACCURACY AND LOSS**\n",
        "\n",
        "1.   MLP : Accuracy : 58%, Loss : 0.0097\n",
        "2.   CNN : Accuracy : 78%, Loss : 0.0052\n",
        "3.   VGG : Accuracy : 86%, Loss : 0.0038\n",
        "\n"
      ],
      "metadata": {
        "id": "KaJzBxiY5m6g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIXdyAFsNpL7"
      },
      "source": [
        "**Reasons Behind Differences in Performance:**\n",
        "\n",
        "\n",
        "*   CNNs leverage the spatial structure of images through convolutional layers, which can capture local patterns and spatial hierarchies. This allows CNNs to learn hierarchical representations of features, from simple to complex, which are crucial for image classification tasks.\n",
        "*  MLPs, on the other hand, treat images as flattened vectors, disregarding their spatial structure. This results in a loss of spatial information and makes it harder for MLPs to learn meaningful representations of images.\n",
        "\n",
        "*  The VGG-based model, being a deep CNN architecture pre-trained on ImageNet, has already learned a rich set of features from a diverse range of images. Fine-tuning this model on CIFAR-10 allows it to adapt these learned features to the specific characteristics of the CIFAR-10 dataset, resulting in better performance compared to training from scratch.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cf41vxSiuek"
      },
      "source": [
        "**Benefits of Transfer Learning:**\n",
        "\n",
        "\n",
        "*   Transfer learning allows us to leverage knowledge gained from training on a large dataset (like ImageNet) and apply it to a different but related task (CIFAR-10).\n",
        "*  When fine-tuning a pre-trained model, unfreezing only the last few layers typically requires less time compared to adding and training a new last layer. Freezing most of the model's layers leverages the pre-existing knowledge, reducing the computational burden and training time significantly.\n",
        "*   The VGG-based model, which utilizes transfer learning, achieved higher accuracy than the MLP and CNN models. This demonstrates the effectiveness of transfer learning in improving performance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tnez5hKGioaQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}